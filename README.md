# Script de Automa√ß√£o de Pesquisa GitHub + SonarQube

Este projeto automatiza a pesquisa comparativa entre projetos de software open-source com **Rapid Release Cycles (RRCs)** e **Slow Releases**, coletando dados do GitHub via API GraphQL/REST e executando an√°lises de qualidade de c√≥digo com SonarQube.

### üèóÔ∏è Arquitetura: Sistema Modular em Duas Fases

#### **Fase 1: Coleta de Reposit√≥rios** (`1_collect_repositories.py`)

- Busca reposit√≥rios no GitHub via API
- Filtra por crit√©rios (stars, forks, releases, contribuidores)
- Classifica como Rapid ou Slow
- Salva dataset em `repositories_dataset.json`
- **Vantagem**: Evita re-consultar a API GitHub a cada an√°lise

#### **Fase 2: An√°lise SonarQube** (`2_analyze_sonarqube.py` ou `analyze_csv_repos.py`)

- Clona reposit√≥rios temporariamente
- Executa SonarScanner via Docker
- Extrai m√©tricas das tabelas do pr√≥prio SonarQube
- Atualiza dataset (JSON ou CSV)
- **Vantagem**: An√°lise incremental com recupera√ß√£o de falhas

### üìã Pr√©-requisitos

#### Software Necess√°rio

- **Docker Desktop** (em execu√ß√£o) - para SonarQube
- **Python 3.7+**
- **Git**

#### Tokens de API

- **GitHub Personal Access Token** com permiss√µes de leitura de reposit√≥rios
- **SonarQube Token** (gerado ap√≥s configura√ß√£o inicial)

> ‚ö†Ô∏è **Nota**: O SonarScanner CLI **n√£o** precisa ser instalado localmente. O script usa a imagem Docker oficial `sonarsource/sonar-scanner-cli`.

### üöÄ Instala√ß√£o e Configura√ß√£o

#### 1. Clone e Configure o Ambiente

```bash
# Clone ou prepare o diret√≥rio
cd script-TI6

# Instale depend√™ncias Python
pip install -r requirements.txt
```

#### 2. Configure Vari√°veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```bash
# GitHub API Configuration
GITHUB_TOKEN=seu_token_github_aqui

# SonarQube Configuration
SONAR_HOST=http://localhost:9000
SONAR_TOKEN=seu_token_sonar_aqui

# Database Configuration (opcional - o SonarQube gerencia suas pr√≥prias tabelas)
DB_HOST=localhost
DB_NAME=sonar
DB_USER=sonar
DB_PASSWORD=sonar
DB_PORT=5432
```

> **Nota**: O PostgreSQL √© usado apenas pelo SonarQube. Os dados da pesquisa s√£o salvos em JSON/CSV.

#### 3. Inicie os Servi√ßos

```bash
# Inicia SonarQube e PostgreSQL
docker-compose up -d

# Verifica se os servi√ßos est√£o rodando
docker-compose ps
```

#### 4. Configure o SonarQube

1. Acesse http://localhost:9000
2. Login inicial: `admin/admin`
3. Altere a senha quando solicitado
4. V√° em **Administration > Security > Users**
5. Clique em **Tokens** para o usu√°rio admin
6. Gere um novo token e copie para a vari√°vel `SONAR_TOKEN` no `.env`

#### 5. Verifica√ß√£o Inicial

Execute testes para verificar o ambiente:

```bash
# Testa conex√£o com SonarQube
python tests/test_sonar_docker.py

# Testa configura√ß√£o Python
python tests/test_platform_compatibility.py
```

### üîß Uso

#### Workflow Completo (JSON)

```bash
# FASE 1: Coleta reposit√≥rios do GitHub
python 1_collect_repositories.py --rapid 50 --slow 50 --max-search 2000

# FASE 2: Analisa com SonarQube (paralelizado)
python 2_analyze_sonarqube.py --workers 4 --skip-analyzed
```

#### Workflow com CSV Pr√©-Coletado

Se voc√™ j√° tem um CSV com reposit√≥rios:

```bash
# Analisa reposit√≥rios do CSV
python analyze_csv_repos.py --csv slow_release_repos.csv --workers 4 --limit 50

# Retoma an√°lise ap√≥s falhas
python analyze_csv_repos.py --csv slow_release_repos.csv --workers 4 --skip-analyzed
```

#### Detalhamento das Fases

##### **Fase 1: Coleta (`1_collect_repositories.py`)**

```bash
# Coleta 100 rapid + 100 slow
python 1_collect_repositories.py --rapid 100 --slow 100

# Expande dataset existente (adiciona mais)
python 1_collect_repositories.py --rapid 200 --slow 200 --max-search 5000
```

**O que faz:**

- Busca reposit√≥rios via API REST do GitHub
- Filtra por: stars ‚â•50, forks ‚â•50, contributors ‚â•19, releases ‚â•19
- Classifica como **Rapid** (5-35 dias entre releases) ou **Slow** (‚â•60 dias)
- Salva em `repositories_dataset.json`
- **N√£o faz** clonagem ou an√°lise (r√°pido e reutiliz√°vel)

##### **Fase 2: An√°lise (`2_analyze_sonarqube.py`)**

```bash
# Analisa todos os reposit√≥rios do dataset
python 2_analyze_sonarqube.py --workers 4

# Analisa apenas tipo espec√≠fico
python 2_analyze_sonarqube.py --type rapid --limit 20 --workers 2

# Pula j√° analisados (recupera√ß√£o de falhas)
python 2_analyze_sonarqube.py --skip-analyzed --workers 4
```

**O que faz:**

1. Clona reposit√≥rios em diret√≥rio tempor√°rio (`%TEMP%\repos_analise\`)
2. Verifica tamanho (pula se >2GB)
3. Executa SonarScanner via Docker: `docker run sonarsource/sonar-scanner-cli`
4. Extrai 13 m√©tricas das **tabelas do pr√≥prio SonarQube**
5. Atualiza dataset JSON/CSV
6. Limpa diret√≥rio tempor√°rio automaticamente

**Paraleliza√ß√£o:**

- `--workers 4`: Analisa 4 repos simultaneamente
- Recomendado: 4 workers em m√°quina com 8GB RAM e SSD

### üìä Dados Coletados e M√©tricas

#### Persist√™ncia de Dados

O sistema usa **JSON/CSV como fonte √∫nica de verdade**, gerenciado pelo `DatasetManager`:

```python
from dataset_manager import DatasetManager

# Modo JSON
dm = DatasetManager("repositories_dataset.json")
repos = dm.get_repositories(release_type='rapid', limit=10)

# Modo CSV (auto-detectado por extens√£o .csv)
dm_csv = DatasetManager("slow_release_repos.csv")
repos = dm_csv.get_repositories()
```

**Formato CSV esperado:**

```
owner,name,stars,forks,language,release_count,contributors,median_release_interval,release_type,reason
```

**Persist√™ncia incremental**: Quando existe `*_analyzed.csv`, carrega dele ao inv√©s do CSV original, preservando an√°lises anteriores.

#### M√©tricas do SonarQube (13 M√©tricas)

As m√©tricas s√£o extra√≠das diretamente das **tabelas do pr√≥prio SonarQube** (n√£o criamos tabelas customizadas):

| M√©trica                    | Descri√ß√£o                                  |
| -------------------------- | ------------------------------------------ |
| `bugs`                     | N√∫mero de bugs detectados                  |
| `vulnerabilities`          | N√∫mero de vulnerabilidades de seguran√ßa    |
| `code_smells`              | N√∫mero de code smells                      |
| `sqale_index`              | D√©bito t√©cnico (em minutos)                |
| `coverage`                 | Cobertura de testes (%)                    |
| `duplicated_lines_density` | Densidade de linhas duplicadas (%)         |
| `ncloc`                    | Linhas de c√≥digo (sem coment√°rios/brancos) |
| `complexity`               | Complexidade ciclom√°tica                   |
| `cognitive_complexity`     | Complexidade cognitiva                     |
| `reliability_rating`       | Rating de confiabilidade (A-E)             |
| `security_rating`          | Rating de seguran√ßa (A-E)                  |
| `sqale_rating`             | Rating de manutenibilidade (A-E)           |
| `alert_status`             | Status do Quality Gate (OK/ERROR)          |

#### Consultar Dados

```bash
# Estat√≠sticas do dataset
python -c "from dataset_manager import DatasetManager; DatasetManager().print_statistics()"

# Exportar para CSV
python -c "from dataset_manager import DatasetManager; DatasetManager().export_to_csv('results.csv')"
```

#### Acesso Direto ao SonarQube

Interface web: http://localhost:9000

- Login: `admin` (senha configurada na instala√ß√£o)
- Visualize projetos analisados em **Projects**
- M√©tricas detalhadas por projeto

### ‚öôÔ∏è Configura√ß√µes Avan√ßadas

#### Crit√©rios de Classifica√ß√£o

Edit√°vel em `1_collect_repositories.py`:

```python
# Rapid Release: 5-35 dias entre releases
if 5 <= avg_interval <= 35 and release_count >= 19 and contributors >= 19:
    return 'rapid'

# Slow Release: ‚â•60 dias entre releases
if avg_interval >= 60 and release_count >= 19 and contributors >= 19:
    return 'slow'
```

#### Ajustar Quantidade de Reposit√≥rios

```bash
# Coletar mais reposit√≥rios
python 1_collect_repositories.py --rapid 500 --slow 500 --max-search 10000
```

#### Timeouts e Paraleliza√ß√£o

No arquivo `2_analyze_sonarqube.py`:

```python
# Timeout para clonagem
timeout=300  # 5 minutos

# Timeout para SonarScanner
timeout=600  # 10 minutos

# N√∫mero de workers paralelos
--workers 4  # Via linha de comando
```

#### Adicionar Linguagens de Programa√ß√£o

Modifique queries em `1_collect_repositories.py`:

```python
search_queries = [
    "stars:>50 forks:>50 language:Python",
    "stars:>50 forks:>50 language:JavaScript",
    "stars:>50 forks:>50 language:Rust",
    # Adicione mais linguagens
]
```

### üêõ Troubleshooting

#### Erro: "Rate limit exceeded" (GitHub API)

- ‚úÖ O script aguarda automaticamente quando o rate limit √© atingido
- GitHub permite 5000 requests/hora para usu√°rios autenticados
- Verifique se o `GITHUB_TOKEN` est√° configurado no `.env`

#### Erro: "Docker n√£o est√° instalado ou n√£o est√° no PATH"

```bash
# Verifique instala√ß√£o
docker --version
docker ps

# Windows: Reinicie o Docker Desktop
# Linux: sudo systemctl restart docker
```

#### Erro: "[WinError 5] Acesso negado" (Windows)

‚úÖ **Corrigido**: O script agora trata permiss√µes de arquivos Git automaticamente via `handle_remove_readonly()`.

Se persistir:

```powershell
# Execute como Administrador
Remove-Item -Recurse -Force $env:TEMP\repos_analise
```

#### Erro de Conex√£o com SonarQube

```bash
# Verifique status dos containers
docker-compose ps

# Teste acesso
curl http://localhost:9000/api/system/status

# Visualize logs
docker-compose logs sonarqube
```

Se o SonarQube n√£o responder, aguarde ~2 minutos para inicializa√ß√£o completa.

#### Reposit√≥rio >2GB Pulado Automaticamente

‚úÖ **Comportamento esperado**: O script pula automaticamente reposit√≥rios maiores que 2GB para evitar consumo excessivo de recursos.

#### An√°lise Falhou para Reposit√≥rio Espec√≠fico

```bash
# Retome an√°lise pulando j√° conclu√≠dos
python 2_analyze_sonarqube.py --skip-analyzed --workers 4

# Para CSV
python analyze_csv_repos.py --csv repos.csv --skip-analyzed --workers 4
```

#### Espa√ßo em Disco

- **Windows**: `%TEMP%\repos_analise\owner_name_{worker_id}`
- **Linux/macOS**: `/tmp/repos_analise/owner_name_{worker_id}`
- Limpeza autom√°tica ap√≥s an√°lise
- Limpeza manual se necess√°rio:

```bash
# Windows PowerShell
Remove-Item -Recurse -Force $env:TEMP\repos_analise

# Linux/macOS
rm -rf /tmp/repos_analise/
```

#### Verifica√ß√£o de Integridade

```bash
# Testa SonarQube via Docker
python tests/test_sonar_docker.py

# Testa compatibilidade do sistema
python tests/test_platform_compatibility.py

# Valida CSV/JSON
python tests/test_incremental_csv.py
```

### üìà Monitoramento e Performance

#### Progresso em Tempo Real

O `ProgressTracker` exibe status durante an√°lise paralela:

```
[15/100] ‚úÖ kubernetes/kubernetes - Conclu√≠do
[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë] 75% | ‚úÖ 70 | ‚ùå 5 | ETA: 12.3min
```

#### Logs dos Servi√ßos

```bash
# SonarQube
docker-compose logs -f sonarqube

# PostgreSQL
docker-compose logs -f db
```

#### Interface SonarQube

- **URL**: http://localhost:9000
- **Login**: `admin` (senha configurada na primeira execu√ß√£o)
- Visualize todos os projetos analisados
- M√©tricas detalhadas por reposit√≥rio

#### Estat√≠sticas do Dataset

```bash
# Status dos reposit√≥rios
python -c "from dataset_manager import DatasetManager; DatasetManager().print_statistics()"

# Exemplo de sa√≠da:
# Total: 100 repos
# Rapid: 50 (40 analisados)
# Slow: 50 (35 analisados)
# Pendentes: 15
```

### üîß Manuten√ß√£o

#### Backup de Dados

```bash
# Backup do dataset JSON
cp repositories_dataset.json repositories_dataset_backup_$(date +%Y%m%d).json

# Backup do banco SonarQube
docker exec sonarqube_db pg_dump -U sonar sonar > sonarqube_backup.sql

# Backup de CSV analisados
cp *_analyzed.csv backups/
```

#### Limpeza de Projetos no SonarQube

```bash
# Acesse: http://localhost:9000/admin/projects_management
# Ou via API:
curl -u admin:sua_senha -X POST "http://localhost:9000/api/projects/delete?project=owner_name"
```

#### Restart dos Servi√ßos

```bash
# Reinicia tudo
docker-compose restart

# Apenas SonarQube
docker-compose restart sonarqube
```

#### Re-analisar Reposit√≥rio Espec√≠fico

```python
from dataset_manager import DatasetManager

# Marca como n√£o analisado
dm = DatasetManager()
dataset = dm.load_dataset()
for repo in dataset['repositories']:
    if repo['full_name'] == 'owner/name':
        repo['sonarqube_analyzed'] = False
dm.save_dataset(dataset)

# Depois execute:
# python 2_analyze_sonarqube.py --skip-analyzed
```

### üìÇ Estrutura de Arquivos

```
1_collect_repositories.py       # Fase 1: Coleta reposit√≥rios do GitHub
2_analyze_sonarqube.py          # Fase 2: An√°lise SonarQube (entrada JSON)
analyze_csv_repos.py            # Fase 2: An√°lise SonarQube (entrada CSV)
dataset_manager.py              # Gerenciador de persist√™ncia (JSON/CSV)
utils.py                        # GitHubAPI, SonarQubeAPI, helpers
docker-compose.yml              # SonarQube + PostgreSQL
repositories_dataset.json       # Dataset principal (gerado por script 1)
*_analyzed.csv                  # Resultados de an√°lise (gerado por CSV)
tests/                          # Testes de valida√ß√£o
```

**Arquivo Legado**: `research_automation_script.py` (monol√≠tico, deprecated)

### üìä Performance

#### Coleta (Fase 1)

- 100 reposit√≥rios ‚âà 200-300 chamadas de API
- Tempo estimado: 30-60 minutos
- Rate limit: 5000 requests/hora (GitHub autenticado)

#### An√°lise (Fase 2)

- **Sequencial**: ~5-10 minutos por reposit√≥rio
- **Paralelo (4 workers)**: ~2-3 minutos por reposit√≥rio
- **Mem√≥ria**: ~500MB por worker
- **Limite**: Reposit√≥rios >2GB s√£o automaticamente pulados

#### Otimiza√ß√£o

```bash
# M√°quina com 8GB RAM + SSD
python 2_analyze_sonarqube.py --workers 4

# M√°quina com 16GB RAM + SSD
python 2_analyze_sonarqube.py --workers 8

# An√°lise limitada (teste)
python 2_analyze_sonarqube.py --limit 10 --workers 2
```

### üß™ Testes

Execute antes de mudan√ßas cr√≠ticas:

```bash
# Valida l√≥gica de classifica√ß√£o
python tests/test_filters.py

# Testa Docker + SonarQube
python tests/test_sonar_docker.py

# Compatibilidade Windows/Linux
python tests/test_platform_compatibility.py

# Persist√™ncia incremental CSV
python tests/test_incremental_csv.py
```

### üìö Documenta√ß√£o Adicional

- [RECOVERY_GUIDE.md](RECOVERY_GUIDE.md) - Recupera√ß√£o de an√°lises falhadas
- [README_CSV.md](README_CSV.md) - Guia espec√≠fico para an√°lise via CSV
- [PROTECOES_SONARQUBE.md](PROTECOES_SONARQUBE.md) - Seguran√ßa e limites

### üìÑ Licen√ßa

Este projeto foi desenvolvido para fins de pesquisa acad√™mica.

### ü§ù Contribui√ß√£o

Para melhorias ou corre√ß√µes:

1. Documente o problema encontrado
2. Teste a solu√ß√£o proposta
3. Execute os testes de verifica√ß√£o
4. Considere impacto em rate limits e performance
5. Mantenha separa√ß√£o entre Fase 1 (coleta) e Fase 2 (an√°lise)

---

**‚ö†Ô∏è Notas Importantes**:

- Este sistema realiza an√°lises intensivas. Monitore CPU, mem√≥ria e disco durante execu√ß√£o.
- Scripts de coleta **nunca** clonam reposit√≥rios.
- Scripts de an√°lise **nunca** buscam no GitHub.
- `DatasetManager` √© a √∫nica fonte de verdade para estado dos reposit√≥rios.
- SonarQube gerencia suas pr√≥prias tabelas - n√£o criamos tabelas customizadas.
